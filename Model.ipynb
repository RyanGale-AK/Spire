{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example cough\n",
    "train_path = 'Freesound_Audio_Train/'\n",
    "file = '02d6b747.wav'\n",
    "sample_rate, samples = wavfile.read(str(train_path) + file)\n",
    "tr_labels = pd.read_csv('train_labels.csv')\n",
    "ts_labels = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Log of spectrogram is much cleaner for plotting and is connected to how humans hear.\n",
    "see https://www.kaggle.com/davids1992/speech-representation-and-data-exploration\"\"\"\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44100\n",
      "\n",
      " 96138\n",
      "0.45871559633027525\n"
     ]
    }
   ],
   "source": [
    "print(sample_rate)\n",
    "print('\\n ' + str(len(samples)))\n",
    "print(sample_rate/len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Visualizing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import *\n",
    "from datetime import datetime\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "#from torchaudio.transforms import MFCC\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our freesound dataset into cough, 1, and non-cough, 0 labels\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "test_labels = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,item in enumerate(train_labels.iloc[:,1].drop_duplicates()):\n",
    "    if idx == 0:\n",
    "        train_non_cough_idx = train_labels[train_labels.iloc[:,1] == item].index.values[:6]\n",
    "    if item != 'Cough':\n",
    "        train_non_cough_idx = np.append(train_non_cough_idx,train_labels[train_labels.iloc[:,1] == item].index.values[:6])\n",
    "        \n",
    "for idx,item in enumerate(test_labels.iloc[:,1].drop_duplicates()):\n",
    "    if idx == 0:\n",
    "        test_non_cough_idx = test_labels[test_labels.iloc[:,1] == item].index.values[:6]\n",
    "    if item != 'Cough':\n",
    "        test_non_cough_idx = np.append(test_non_cough_idx,test_labels[test_labels.iloc[:,1] == item].index.values[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_non_cough_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cough_idx = train_labels[train_labels.iloc[:,1] == 'Cough'].index.values\n",
    "test_cough_idx = test_labels[test_labels.iloc[:,1] == 'Cough'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = np.zeros((489,5)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_non_idx = train_labels.copy().iloc[train_non_cough_idx,1][:246].index.values\n",
    "train_non_cough_labels = train_labels.copy().iloc[train_non_idx,:]\n",
    "train_non_cough_labels.iloc[:,1] = 0\n",
    "train_cough_labels = train_labels.copy().iloc[train_cough_idx,:]\n",
    "train_cough_labels.iloc[:,1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_non_idx = test_labels.copy().iloc[test_non_cough_idx,1][:30].index.values\n",
    "test_non_cough_labels = test_labels.copy().iloc[test_non_idx,:]\n",
    "test_non_cough_labels.iloc[:,1] = 0\n",
    "test_cough_labels = test_labels.copy().iloc[test_cough_idx,:]\n",
    "test_cough_labels.iloc[:,1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pd.concat([train_non_cough_labels,train_cough_labels])\n",
    "new_test = pd.concat([test_non_cough_labels,test_cough_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.to_csv('train_binary_labels.csv', index=False)\n",
    "new_test.to_csv('test_binary_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreesoundDataset(Dataset):\n",
    "    def __init__(self, csv_labels, root_dir, transform=None, test=False, evaluate=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_labels: path to labels file\n",
    "            root_dir: path to file directory\n",
    "            transform: optional transforms\n",
    "            test: training vs testing mode\n",
    "        \"\"\"\n",
    "        self.labels = pd.read_csv(csv_labels)\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "        self.evaluate = evaluate\n",
    "        \n",
    "        # root path depends on training vs testing data files\n",
    "        if self.test:\n",
    "            self.root = os.path.join(root_dir,'Freesound_Audio_Test')\n",
    "        else:\n",
    "            self.root = os.path.join(root_dir,'Freesound_Audio_Train')\n",
    "\n",
    "        if self.evaluate:\n",
    "            self.root = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # start_time = datetime.now()\n",
    "        filename = self.labels.iloc[idx,0]\n",
    "        wave_path = os.path.join(self.root,filename)\n",
    "        sample_rate, sample = wavfile.read(wave_path)\n",
    "        \n",
    "        # clamp all samples to 500,000 frames\n",
    "        if len(sample) > 500000:\n",
    "            sample = sample[:500000]\n",
    "        else:\n",
    "            l = len(sample)\n",
    "            temp = np.zeros(500000,dtype='int16')\n",
    "            temp[0:l] = sample\n",
    "            sample = temp\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        # reshape to (3,224,224) for \n",
    "        temp = torch.tensor(sample.numpy().reshape(1,1,128,977))\n",
    "        downsample = torch.nn.functional.interpolate(temp, (224, 224), mode='bilinear')\n",
    "        temp = np.repeat(downsample[...],3, axis=1)\n",
    "        sample = temp.squeeze()\n",
    "        \n",
    "        normalize = transforms.Compose([transforms.Normalize(mean,std)])\n",
    "        sample = normalize(sample)\n",
    "        #print(\"Time Elapsed Processing WAV: {} seconds\".format(\n",
    "        #(datetime.now() - start_time).total_seconds()))\n",
    "        \n",
    "        label = self.labels.iloc[idx,1]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.485, 0.456, 0.406]\n"
     ]
    }
   ],
   "source": [
    "# std = (0.229 + 0.224 + 0.225) / 3\n",
    "# mean = (0.485 + 0.456 + 0.406) / 3\n",
    "std = [0.229, 0.224, 0.225]\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "print(mean)\n",
    "\"\"\"Mel-frequency cepstral coefficients form gives us\n",
    "    approximation of human auditory response.\n",
    "    This also attempts to remove speaker dependent qualities.\n",
    "    our sample rate is 44100 after reading the wav files,\n",
    "    start with 20 mfccs then increase if needed\"\"\"\n",
    "trns = transforms.Compose([\n",
    "    lambda x: x.astype(np.float32) / np.max(x),\n",
    "    lambda x: librosa.feature.mfcc(x, sr=44100, n_mfcc=128),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=trns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.6154e+02,  2.9200e+02,  1.2695e+02,  ..., -2.3144e+03,\n",
       "           -2.3212e+03, -2.3359e+03],\n",
       "          [ 1.9039e+02,  1.6011e+02,  5.2380e+01,  ..., -1.4520e+03,\n",
       "           -1.4552e+03, -1.4679e+03],\n",
       "          [ 7.6568e+01, -5.0923e+01, -6.6926e+01,  ..., -7.2118e+01,\n",
       "           -6.9787e+01, -7.9184e+01],\n",
       "          ...,\n",
       "          [ 9.7699e+00,  4.0477e+00, -1.4634e+00,  ..., -3.9907e+00,\n",
       "           -7.3203e+00, -8.2276e-02],\n",
       "          [ 4.3810e+00, -3.6112e+00, -2.2273e-01,  ..., -2.3093e+00,\n",
       "           -7.3517e+00, -2.2037e+00],\n",
       "          [ 1.0130e+00, -8.3979e+00,  5.5268e-01,  ..., -1.2584e+00,\n",
       "           -7.3714e+00, -3.5295e+00]],\n",
       " \n",
       "         [[ 2.6750e+02,  2.9865e+02,  1.2991e+02,  ..., -2.3659e+03,\n",
       "           -2.3728e+03, -2.3879e+03],\n",
       "          [ 1.9477e+02,  1.6381e+02,  5.3679e+01,  ..., -1.4843e+03,\n",
       "           -1.4876e+03, -1.5006e+03],\n",
       "          [ 7.8406e+01, -5.1930e+01, -6.8290e+01,  ..., -7.3598e+01,\n",
       "           -7.1215e+01, -8.0822e+01],\n",
       "          ...,\n",
       "          [ 1.0117e+01,  4.2675e+00, -1.3666e+00,  ..., -3.9503e+00,\n",
       "           -7.3542e+00,  4.5352e-02],\n",
       "          [ 4.6083e+00, -3.5623e+00, -9.8241e-02,  ..., -2.2314e+00,\n",
       "           -7.3864e+00, -2.1234e+00],\n",
       "          [ 1.1651e+00, -8.4559e+00,  6.9448e-01,  ..., -1.1570e+00,\n",
       "           -7.4065e+00, -3.4788e+00]],\n",
       " \n",
       "         [[ 2.6654e+02,  2.9754e+02,  1.2955e+02,  ..., -2.3552e+03,\n",
       "           -2.3621e+03, -2.3771e+03],\n",
       "          [ 1.9413e+02,  1.6330e+02,  5.3662e+01,  ..., -1.4775e+03,\n",
       "           -1.4808e+03, -1.4937e+03],\n",
       "          [ 7.8280e+01, -5.1477e+01, -6.7765e+01,  ..., -7.3049e+01,\n",
       "           -7.0676e+01, -8.0240e+01],\n",
       "          ...,\n",
       "          [ 1.0295e+01,  4.4708e+00, -1.1383e+00,  ..., -3.7105e+00,\n",
       "           -7.0993e+00,  2.6737e-01],\n",
       "          [ 4.8100e+00, -3.3242e+00,  1.2442e-01,  ..., -1.9992e+00,\n",
       "           -7.1313e+00, -1.8917e+00],\n",
       "          [ 1.3821e+00, -8.1961e+00,  9.1361e-01,  ..., -9.2967e-01,\n",
       "           -7.1514e+00, -3.2412e+00]]]), 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what our samples look like\n",
    "#plt.plot(example[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING CODE\n",
    "# # reshape spectrograms to fit (3,224,224) size tensor\n",
    "# print(example.shape)\n",
    "# temp = torch.tensor(example.numpy().reshape(1,1,128,977))\n",
    "# print(temp.shape)\n",
    "# downsample = torch.nn.functional.interpolate(temp, (224, 224), mode='bilinear')\n",
    "# print(downsample.shape)\n",
    "# print(\"downsample \\n\" + str(downsample.numpy()[0]))\n",
    "# temp = np.repeat(downsample[...],3, axis=1)\n",
    "# print(temp.shape)\n",
    "# print(\"reshaped \\n\" + str(temp[0]))\n",
    "# final = temp.squeeze()\n",
    "# print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=trns), batch_size=10, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 224, 224)\n",
      "(10, 3, 224, 224)\n",
      "(10, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "for idx, (sample, label) in enumerate(dataloader):\n",
    "    print(sample.numpy().shape)\n",
    "    #plt.imshow(sample.numpy()[0,:,:])\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build net and preprocess inputs for ResNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pretrainedmodels\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fbresnet152', 'bninception', 'resnext101_32x4d', 'resnext101_64x4d', 'inceptionv4', 'inceptionresnetv2', 'alexnet', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'inceptionv3', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'nasnetamobile', 'nasnetalarge', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107', 'xception', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'cafferesnet101', 'pnasnet5large', 'polynet']\n"
     ]
    }
   ],
   "source": [
    "print(pretrainedmodels.model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imagenet': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth', 'input_space': 'RGB', 'input_size': [3, 224, 224], 'input_range': [0, 1], 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225], 'num_classes': 1000}}\n"
     ]
    }
   ],
   "source": [
    "print(pretrainedmodels.pretrained_settings['se_resnext50_32x4d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use AlexNet\n",
    "import torchvision.models as models\n",
    "#from sampler import ImbalancedDatasetSampler\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on GPU if we can\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "std = [0.229, 0.224, 0.225]\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    lambda x: x.astype(np.float32) / np.max(x),\n",
    "    lambda x: librosa.feature.mfcc(x, sr=44100, n_mfcc=128),\n",
    "    transforms.ToTensor() \n",
    "    ])\n",
    "train_data = FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=transform)\n",
    "test_data = FreesoundDataset(csv_labels='test_binary_labels.csv',root_dir='.',transform=transform, test=True)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True) # drop last batch if not full\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "net = alexnet\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "new_classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 2),\n",
    "        )\n",
    "\n",
    "net.classifier = new_classifier\n",
    "\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.SGD(net.classifier.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.classifier.parameters(),lr=0.001)\n",
    "#schedule = lr_scheduler.StepLR(optimizer, step_size=10,gamma=0.1)\n",
    "epochs = 30\n",
    "BATCH_SIZE = 64\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReNeXt Attempt\n",
    "# # train on GPU if we can\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# std = pretrainedmodels.pretrained_settings['se_resnext50_32x4d'][\"imagenet\"]['std']\n",
    "# mean = pretrainedmodels.pretrained_settings['se_resnext50_32x4d'][\"imagenet\"]['mean']\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     lambda x: x.astype(np.float32) / np.max(x),\n",
    "#     lambda x: librosa.feature.mfcc(x, sr=44100, n_mfcc=128),\n",
    "#     transforms.ToTensor() \n",
    "#     ])\n",
    "\n",
    "# train_dataloader = DataLoader(FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=transform), batch_size=64, shuffle=True, drop_last=True) # drop last batch if not full\n",
    "\n",
    "# resnext = 'se_resnext50_32x4d'\n",
    "# net = pretrainedmodels.__dict__[resnext](num_classes=1000,pretrained='imagenet')\n",
    "# for param in net.parameters():\n",
    "#     param.requires_grad = False\n",
    "# num_ftrs = net.last_linear.in_features\n",
    "# net.last_linear = nn.Linear(num_ftrs, 2)\n",
    "# net = net.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.SGD(net.last_linear.parameters(), lr=0.001, momentum=0.9)\n",
    "# schedule = lr_scheduler.StepLR(optimizer, step_size=10,gamma=0.1)\n",
    "# epochs = 3 \n",
    "# BATCH_SIZE = 64\n",
    "# use_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(net, inputs, labels, criterion, optimizer):\n",
    "    \"\"\"\n",
    "        net        network used to train\n",
    "        inputs     (torch Tensor) batch of input images\n",
    "        labels     (torch Tensor) ground truth labels\n",
    "        criterion   loss function\n",
    "        optimizer  used in backward pass\n",
    "\n",
    "    Returns:\n",
    "        running_loss    (float) loss from this batch of images\n",
    "        num_correct     (torch Tensor, size 1) number of inputs\n",
    "                        in this batch predicted correctly\n",
    "        total_images    (float or int) total number of images in this batch\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with torch.set_grad_enabled(True):\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "    \n",
    "    # back pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, index_out = outputs.max(1)\n",
    "    running_loss = loss.item()\n",
    "    num_correct = sum(index_out.squeeze() == labels.squeeze()).item()\n",
    "    total_images = inputs.size(0)\n",
    "\n",
    "    return running_loss, num_correct, total_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the params of our net\n",
    "# !pip install torchsummary\n",
    "# from torchsummary import summary\n",
    "# summary(net, input_size=(2,64, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(curr_batch, batch_size, curr_epoch, total_epochs, dataloader):\n",
    "    \"\"\"\n",
    "    Prints message logging progress through training.\n",
    "    \"\"\"\n",
    "    progress = float(curr_batch + 1)/(float(len(dataloader.dataset)) / batch_size)\n",
    "    log = \"EPOCH [{}/{}].Progress: {} % \".format(\n",
    "        curr_epoch + 1, total_epochs, round(progress * 100, 2))\n",
    "    sys.stdout.write(\"\\r\" + log)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # test dataloader\n",
    "# for batch_num, (test_inputs, test_labels) in enumerate(test_dataloader):\n",
    "#     print(\"batch_num = \" + str(batch_num))\n",
    "#     print(\"label = \" + str(test_labels))\n",
    "#     print(test_inputs)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "EPOCH [1/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [9.9602] Acc: 0.5 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [2.3484] Acc: 0.53125 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [2/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.4409] Acc: 0.49776785714285715 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.3457] Acc: 0.53125 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [3/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0973] Acc: 0.5357142857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0172] Acc: 0.59375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [4/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0174] Acc: 0.5558035714285714 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0184] Acc: 0.625 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [5/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0114] Acc: 0.6227678571428571 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0186] Acc: 0.4375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [6/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0093] Acc: 0.6696428571428571 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0198] Acc: 0.53125 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [7/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0088] Acc: 0.6919642857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0158] Acc: 0.875 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [8/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0081] Acc: 0.7254464285714286 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0143] Acc: 0.9375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [9/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0077] Acc: 0.7299107142857143 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0131] Acc: 0.90625 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [10/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0078] Acc: 0.7566964285714286 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0118] Acc: 0.90625 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [11/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0075] Acc: 0.7544642857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0122] Acc: 0.78125 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [12/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0076] Acc: 0.7544642857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0115] Acc: 0.875 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [13/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0072] Acc: 0.7790178571428571 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0117] Acc: 0.9375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [14/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0071] Acc: 0.7857142857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0129] Acc: 1.0 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [15/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0073] Acc: 0.7589285714285714 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0115] Acc: 0.90625 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [16/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0065] Acc: 0.796875 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0095] Acc: 0.90625 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [17/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0069] Acc: 0.7857142857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0101] Acc: 0.96875 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [18/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0066] Acc: 0.8102678571428571 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0105] Acc: 0.84375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [19/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0061] Acc: 0.8169642857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0104] Acc: 0.875 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [20/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0068] Acc: 0.8035714285714286 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0097] Acc: 0.9375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [21/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0058] Acc: 0.8147321428571429 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0086] Acc: 0.96875 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [22/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0061] Acc: 0.796875 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0074] Acc: 0.9375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [23/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0058] Acc: 0.8303571428571429 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0071] Acc: 0.9375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [24/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.006] Acc: 0.8058035714285714 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.008] Acc: 0.96875 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [25/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0051] Acc: 0.859375 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0078] Acc: 0.9375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [26/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0057] Acc: 0.8258928571428571 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.009] Acc: 0.96875 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [27/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0056] Acc: 0.8258928571428571 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0088] Acc: 0.96875 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [28/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0057] Acc: 0.8169642857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0085] Acc: 0.96875 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [29/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0054] Acc: 0.8348214285714286 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0084] Acc: 0.9375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [30/30].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.005] Acc: 0.8571428571428571 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0067] Acc: 0.96875 on 32.0 images\n",
      "\n",
      "Time Elapsed: 1883.171688 seconds\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = np.zeros(epochs)\n",
    "train_acc_history  = np.zeros(epochs)\n",
    "val_loss_history   = np.zeros(epochs)\n",
    "val_acc_history    = np.zeros(epochs)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # TRAINING ###\n",
    "    print(\"Training...\")\n",
    "    net.train()\n",
    "    \n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "\n",
    "    for batch_num, (inputs, labels) in enumerate(train_dataloader):\n",
    "        #print(labels)\n",
    "        log_progress(batch_num, BATCH_SIZE, epoch, epochs, train_dataloader)\n",
    "\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        assert inputs.shape == (64,3,224,224), inputs.shape\n",
    "        curr_loss, curr_correct, curr_images = model_train(net, inputs, labels, criterion, optimizer)\n",
    "        running_loss += curr_loss\n",
    "        num_correct += curr_correct\n",
    "        total_images += curr_images\n",
    "\n",
    "    # Update statistics for epoch\n",
    "    train_loss_history[epoch] = running_loss / total_images\n",
    "    train_acc_history[epoch]  = float(num_correct)  / float(total_images)\n",
    "    print(\"\\n Train Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "          round(train_loss_history[epoch],4), train_acc_history[epoch], total_images) )\n",
    "    \n",
    "    # VALIDATION ###\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    net.eval()\n",
    "    \n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    for batch_num, (test_inputs, test_labels) in enumerate(test_dataloader):\n",
    "        \n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            test_inputs = test_inputs.cuda()\n",
    "            test_labels = test_labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs  = net(test_inputs)\n",
    "                                                 \n",
    "        loss     = criterion(outputs, test_labels.squeeze())\n",
    "                                                 \n",
    "        # Prediction is index with highest class score\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # move data back for analysis\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            test_labels = test_labels.cpu()\n",
    "            preds = preds.cpu()\n",
    "\n",
    "        running_loss  += loss.item()\n",
    "        num_correct   += torch.sum(preds == test_labels.data.reshape(-1))\n",
    "\n",
    "        total_images  += test_labels.data.numpy().size\n",
    "        \n",
    "    # Update stats for validation data\n",
    "    val_loss_history[epoch] = running_loss / total_images\n",
    "    val_acc_history[epoch]  = float(num_correct)  / float(total_images) \n",
    "    print(\"Val Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "        round(val_loss_history[epoch],4), val_acc_history[epoch], total_images))\n",
    "    \n",
    "print(\"Time Elapsed: {} seconds\".format(\n",
    "    (datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cough_net = net\n",
    "cough_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_non_cough_idx = train_labels[train_labels.iloc[:,1] != 'Cough'].index\n",
    "train_cough_idx = train_labels[train_labels.iloc[:,1] == 'Cough'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0])\n",
      "labels = tensor([0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "cough_net.eval()\n",
    "\n",
    "for batch_num, (test_inputs, test_labels) in enumerate(test_dataloader):\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "            test_inputs = test_inputs.cuda()\n",
    "            test_labels = test_labels.cuda()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs  = net(test_inputs)\n",
    "                                                 \n",
    "    loss     = criterion(outputs, test_labels.squeeze())\n",
    "                                                 \n",
    "    # Prediction is index with highest class score\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    print(\"predictions = \" + str(preds))\n",
    "    print(\"labels = \" + str(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs = 44100  # Sample rate\n",
    "seconds = 3  # Duration of recording\n",
    "\n",
    "myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=1)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "write('output.wav', fs, myrecording)  # Save as WAV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -0.00063076,\n",
       "        0.00011138,  0.00092226], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate, sample = wavfile.read('output.wav')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "sample_rate, sample = wavfile.read('output.wav')\n",
    "sample = sample + 1e-012\n",
    "# start_time = datetime.now()\n",
    "# clamp all samples to 500,000 frames\n",
    "if len(sample) > 500000:\n",
    "    sample = sample[:500000]\n",
    "else:\n",
    "    l = len(sample)\n",
    "    temp = np.zeros(500000,dtype='int16')\n",
    "    temp[0:l] = sample\n",
    "    sample = temp\n",
    "    \n",
    "print(np.isnan(sample).any())\n",
    "sample = transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cough was predicted as...\n",
      "Not a Cough\n"
     ]
    }
   ],
   "source": [
    "sample_rate, sample = wavfile.read('output.wav')\n",
    "sample = sample + 1e-012\n",
    "# start_time = datetime.now()\n",
    "# clamp all samples to 500,000 frames\n",
    "if len(sample) > 500000:\n",
    "    sample = sample[:500000]\n",
    "else:\n",
    "    l = len(sample)\n",
    "    temp = np.zeros(500000,dtype='int16')\n",
    "    temp[0:l] = sample\n",
    "    sample = temp\n",
    "    \n",
    "sample = sample + 1e-012\n",
    "\n",
    "sample = transform(sample)\n",
    "# reshape to (3,224,224) for\n",
    "temp = torch.tensor(sample.numpy().reshape(1,1,128,977))\n",
    "downsample = torch.nn.functional.interpolate(temp, (224, 224), mode='bilinear')\n",
    "temp = np.repeat(downsample[...],3, axis=1)\n",
    "sample = temp.squeeze()\n",
    "normalize = transforms.Compose([transforms.Normalize(mean,std)])\n",
    "sample = normalize(sample)\n",
    "#print(\"Time Elapsed Processing WAV: {} seconds\".format(\n",
    "#(datetime.now() - start_time).total_seconds()))\n",
    "sample.shape\n",
    "'''Not sure if anything from here on works'''\n",
    "live_data = FreesoundDataset(csv_labels = 'live_binary_labels.csv', root_dir='.',transform=transform, test=True, evaluate=True)\n",
    "live_dataloader = DataLoader(live_data, batch_size=1, shuffle=True, drop_last=True)\n",
    "preds = []\n",
    "labels = []\n",
    "for batch_num, (input, label) in enumerate(live_dataloader):\n",
    "    output = net(input.float())\n",
    "    _, pred = torch.max(output, 1)\n",
    "    preds.append(pred)\n",
    "    labels.append(label)\n",
    "    break\n",
    "print(\"Cough was predicted as...\")\n",
    "\n",
    "if pred.numpy()[0]:\n",
    "    print(\"Cough\")\n",
    "else:\n",
    "    print(\"Not a Cough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
