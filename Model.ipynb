{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example cough\n",
    "train_path = 'Freesound_Audio_Train/'\n",
    "file = '02d6b747.wav'\n",
    "sample_rate, samples = wavfile.read(str(train_path) + file)\n",
    "tr_labels = pd.read_csv('train_labels.csv')\n",
    "ts_labels = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Log of spectrogram is much cleaner for plotting and is connected to how humans hear.\n",
    "see https://www.kaggle.com/davids1992/speech-representation-and-data-exploration\"\"\"\n",
    "def log_specgram(audio, sample_rate, window_size=20,\n",
    "                 step_size=10, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    return freqs, times, np.log(spec.T.astype(np.float32) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44100\n",
      "\n",
      " 96138\n",
      "0.45871559633027525\n"
     ]
    }
   ],
   "source": [
    "print(sample_rate)\n",
    "print('\\n ' + str(len(samples)))\n",
    "print(sample_rate/len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Visualizing Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import *\n",
    "from datetime import datetime\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "#from torchaudio.transforms import MFCC\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our freesound dataset into cough, 1, and non-cough, 0 labels\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "test_labels = pd.read_csv('test_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,item in enumerate(train_labels.iloc[:,1].drop_duplicates()):\n",
    "    if idx == 0:\n",
    "        train_non_cough_idx = train_labels[train_labels.iloc[:,1] == item].index.values[:6]\n",
    "    if item != 'Cough':\n",
    "        train_non_cough_idx = np.append(train_non_cough_idx,train_labels[train_labels.iloc[:,1] == item].index.values[:6])\n",
    "        \n",
    "for idx,item in enumerate(test_labels.iloc[:,1].drop_duplicates()):\n",
    "    if idx == 0:\n",
    "        test_non_cough_idx = test_labels[test_labels.iloc[:,1] == item].index.values[:6]\n",
    "    if item != 'Cough':\n",
    "        test_non_cough_idx = np.append(test_non_cough_idx,test_labels[test_labels.iloc[:,1] == item].index.values[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_non_cough_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cough_idx = train_labels[train_labels.iloc[:,1] == 'Cough'].index.values\n",
    "test_cough_idx = test_labels[test_labels.iloc[:,1] == 'Cough'].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = np.zeros((489,5)).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_non_idx = train_labels.copy().iloc[train_non_cough_idx,1][:246].index.values\n",
    "train_non_cough_labels = train_labels.copy().iloc[train_non_idx,:]\n",
    "train_non_cough_labels.iloc[:,1] = 0\n",
    "train_cough_labels = train_labels.copy().iloc[train_cough_idx,:]\n",
    "train_cough_labels.iloc[:,1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_non_idx = test_labels.copy().iloc[test_non_cough_idx,1][:30].index.values\n",
    "test_non_cough_labels = test_labels.copy().iloc[test_non_idx,:]\n",
    "test_non_cough_labels.iloc[:,1] = 0\n",
    "test_cough_labels = test_labels.copy().iloc[test_cough_idx,:]\n",
    "test_cough_labels.iloc[:,1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = pd.concat([train_non_cough_labels,train_cough_labels])\n",
    "new_test = pd.concat([test_non_cough_labels,test_cough_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.to_csv('train_binary_labels.csv', index=False)\n",
    "new_test.to_csv('test_binary_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(489, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreesoundDataset(Dataset):\n",
    "    def __init__(self, csv_labels, root_dir, transform=None, test=False, evaluate=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_labels: path to labels file\n",
    "            root_dir: path to file directory\n",
    "            transform: optional transforms\n",
    "            test: training vs testing mode\n",
    "        \"\"\"\n",
    "        self.labels = pd.read_csv(csv_labels)\n",
    "        self.transform = transform\n",
    "        self.test = test\n",
    "        self.evaluate = evaluate\n",
    "        \n",
    "        # root path depends on training vs testing data files\n",
    "        if self.test:\n",
    "            self.root = os.path.join(root_dir,'Freesound_Audio_Test')\n",
    "        else:\n",
    "            self.root = os.path.join(root_dir,'Freesound_Audio_Train')\n",
    "\n",
    "        if self.evaluate:\n",
    "            self.root = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # start_time = datetime.now()\n",
    "        filename = self.labels.iloc[idx,0]\n",
    "        wave_path = os.path.join(self.root,filename)\n",
    "        sample_rate, sample = wavfile.read(wave_path)\n",
    "        \n",
    "        # clamp all samples to 500,000 frames\n",
    "        if len(sample) > 500000:\n",
    "            sample = sample[:500000]\n",
    "        else:\n",
    "            l = len(sample)\n",
    "            temp = np.zeros(500000,dtype='int16')\n",
    "            temp[0:l] = sample\n",
    "            sample = temp\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        # reshape to (3,224,224) for \n",
    "        temp = torch.tensor(sample.numpy().reshape(1,1,128,977))\n",
    "        downsample = torch.nn.functional.interpolate(temp, (224, 224), mode='bilinear')\n",
    "        temp = np.repeat(downsample[...],3, axis=1)\n",
    "        sample = temp.squeeze()\n",
    "        \n",
    "        normalize = transforms.Compose([transforms.Normalize(mean,std)])\n",
    "        sample = normalize(sample)\n",
    "        #print(\"Time Elapsed Processing WAV: {} seconds\".format(\n",
    "        #(datetime.now() - start_time).total_seconds()))\n",
    "        \n",
    "        label = self.labels.iloc[idx,1]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.485, 0.456, 0.406]\n"
     ]
    }
   ],
   "source": [
    "# std = (0.229 + 0.224 + 0.225) / 3\n",
    "# mean = (0.485 + 0.456 + 0.406) / 3\n",
    "std = [0.229, 0.224, 0.225]\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "print(mean)\n",
    "\"\"\"Mel-frequency cepstral coefficients form gives us\n",
    "    approximation of human auditory response.\n",
    "    This also attempts to remove speaker dependent qualities.\n",
    "    our sample rate is 44100 after reading the wav files,\n",
    "    start with 20 mfccs then increase if needed\"\"\"\n",
    "trns = transforms.Compose([\n",
    "    lambda x: x.astype(np.float32) / np.max(x),\n",
    "    lambda x: librosa.feature.mfcc(x, sr=44100, n_mfcc=128),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=trns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.6154e+02,  2.9200e+02,  1.2695e+02,  ..., -2.3144e+03,\n",
       "           -2.3212e+03, -2.3359e+03],\n",
       "          [ 1.9039e+02,  1.6011e+02,  5.2380e+01,  ..., -1.4520e+03,\n",
       "           -1.4552e+03, -1.4679e+03],\n",
       "          [ 7.6568e+01, -5.0923e+01, -6.6926e+01,  ..., -7.2118e+01,\n",
       "           -6.9787e+01, -7.9184e+01],\n",
       "          ...,\n",
       "          [ 9.7699e+00,  4.0477e+00, -1.4634e+00,  ..., -3.9907e+00,\n",
       "           -7.3203e+00, -8.2276e-02],\n",
       "          [ 4.3810e+00, -3.6112e+00, -2.2273e-01,  ..., -2.3093e+00,\n",
       "           -7.3517e+00, -2.2037e+00],\n",
       "          [ 1.0130e+00, -8.3979e+00,  5.5268e-01,  ..., -1.2584e+00,\n",
       "           -7.3714e+00, -3.5295e+00]],\n",
       " \n",
       "         [[ 2.6750e+02,  2.9865e+02,  1.2991e+02,  ..., -2.3659e+03,\n",
       "           -2.3728e+03, -2.3879e+03],\n",
       "          [ 1.9477e+02,  1.6381e+02,  5.3679e+01,  ..., -1.4843e+03,\n",
       "           -1.4876e+03, -1.5006e+03],\n",
       "          [ 7.8406e+01, -5.1930e+01, -6.8290e+01,  ..., -7.3598e+01,\n",
       "           -7.1215e+01, -8.0822e+01],\n",
       "          ...,\n",
       "          [ 1.0117e+01,  4.2675e+00, -1.3666e+00,  ..., -3.9503e+00,\n",
       "           -7.3542e+00,  4.5352e-02],\n",
       "          [ 4.6083e+00, -3.5623e+00, -9.8241e-02,  ..., -2.2314e+00,\n",
       "           -7.3864e+00, -2.1234e+00],\n",
       "          [ 1.1651e+00, -8.4559e+00,  6.9448e-01,  ..., -1.1570e+00,\n",
       "           -7.4065e+00, -3.4788e+00]],\n",
       " \n",
       "         [[ 2.6654e+02,  2.9754e+02,  1.2955e+02,  ..., -2.3552e+03,\n",
       "           -2.3621e+03, -2.3771e+03],\n",
       "          [ 1.9413e+02,  1.6330e+02,  5.3662e+01,  ..., -1.4775e+03,\n",
       "           -1.4808e+03, -1.4937e+03],\n",
       "          [ 7.8280e+01, -5.1477e+01, -6.7765e+01,  ..., -7.3049e+01,\n",
       "           -7.0676e+01, -8.0240e+01],\n",
       "          ...,\n",
       "          [ 1.0295e+01,  4.4708e+00, -1.1383e+00,  ..., -3.7105e+00,\n",
       "           -7.0993e+00,  2.6737e-01],\n",
       "          [ 4.8100e+00, -3.3242e+00,  1.2442e-01,  ..., -1.9992e+00,\n",
       "           -7.1313e+00, -1.8917e+00],\n",
       "          [ 1.3821e+00, -8.1961e+00,  9.1361e-01,  ..., -9.2967e-01,\n",
       "           -7.1514e+00, -3.2412e+00]]]), 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what our samples look like\n",
    "#plt.plot(example[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING CODE\n",
    "# # reshape spectrograms to fit (3,224,224) size tensor\n",
    "# print(example.shape)\n",
    "# temp = torch.tensor(example.numpy().reshape(1,1,128,977))\n",
    "# print(temp.shape)\n",
    "# downsample = torch.nn.functional.interpolate(temp, (224, 224), mode='bilinear')\n",
    "# print(downsample.shape)\n",
    "# print(\"downsample \\n\" + str(downsample.numpy()[0]))\n",
    "# temp = np.repeat(downsample[...],3, axis=1)\n",
    "# print(temp.shape)\n",
    "# print(\"reshaped \\n\" + str(temp[0]))\n",
    "# final = temp.squeeze()\n",
    "# print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=trns), batch_size=10, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 224, 224)\n",
      "(10, 3, 224, 224)\n",
      "(10, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "for idx, (sample, label) in enumerate(dataloader):\n",
    "    print(sample.numpy().shape)\n",
    "    #plt.imshow(sample.numpy()[0,:,:])\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build net and preprocess inputs for ResNeXt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pretrainedmodels\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fbresnet152', 'bninception', 'resnext101_32x4d', 'resnext101_64x4d', 'inceptionv4', 'inceptionresnetv2', 'alexnet', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'inceptionv3', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'nasnetamobile', 'nasnetalarge', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn131', 'dpn107', 'xception', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'cafferesnet101', 'pnasnet5large', 'polynet']\n"
     ]
    }
   ],
   "source": [
    "print(pretrainedmodels.model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imagenet': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth', 'input_space': 'RGB', 'input_size': [3, 224, 224], 'input_range': [0, 1], 'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225], 'num_classes': 1000}}\n"
     ]
    }
   ],
   "source": [
    "print(pretrainedmodels.pretrained_settings['se_resnext50_32x4d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use AlexNet\n",
    "import torchvision.models as models\n",
    "#from sampler import ImbalancedDatasetSampler\n",
    "alexnet = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on GPU if we can\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "std = [0.229, 0.224, 0.225]\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    lambda x: x.astype(np.float32) / np.max(x),\n",
    "    lambda x: librosa.feature.mfcc(x, sr=44100, n_mfcc=128),\n",
    "    transforms.ToTensor() \n",
    "    ])\n",
    "train_data = FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=transform)\n",
    "test_data = FreesoundDataset(csv_labels='test_binary_labels.csv',root_dir='.',transform=transform, test=True)\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True) # drop last batch if not full\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "net = alexnet\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "new_classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 2),\n",
    "        )\n",
    "\n",
    "net.classifier = new_classifier\n",
    "\n",
    "net = net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.SGD(net.classifier.parameters(), lr=0.0001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.classifier.parameters(),lr=0.001)\n",
    "#schedule = lr_scheduler.StepLR(optimizer, step_size=10,gamma=0.1)\n",
    "epochs = 10\n",
    "BATCH_SIZE = 64\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReNeXt Attempt\n",
    "# # train on GPU if we can\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# std = pretrainedmodels.pretrained_settings['se_resnext50_32x4d'][\"imagenet\"]['std']\n",
    "# mean = pretrainedmodels.pretrained_settings['se_resnext50_32x4d'][\"imagenet\"]['mean']\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     lambda x: x.astype(np.float32) / np.max(x),\n",
    "#     lambda x: librosa.feature.mfcc(x, sr=44100, n_mfcc=128),\n",
    "#     transforms.ToTensor() \n",
    "#     ])\n",
    "\n",
    "# train_dataloader = DataLoader(FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=transform), batch_size=64, shuffle=True, drop_last=True) # drop last batch if not full\n",
    "\n",
    "# resnext = 'se_resnext50_32x4d'\n",
    "# net = pretrainedmodels.__dict__[resnext](num_classes=1000,pretrained='imagenet')\n",
    "# for param in net.parameters():\n",
    "#     param.requires_grad = False\n",
    "# num_ftrs = net.last_linear.in_features\n",
    "# net.last_linear = nn.Linear(num_ftrs, 2)\n",
    "# net = net.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.SGD(net.last_linear.parameters(), lr=0.001, momentum=0.9)\n",
    "# schedule = lr_scheduler.StepLR(optimizer, step_size=10,gamma=0.1)\n",
    "# epochs = 3 \n",
    "# BATCH_SIZE = 64\n",
    "# use_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(net, inputs, labels, criterion, optimizer):\n",
    "    \"\"\"\n",
    "        net        network used to train\n",
    "        inputs     (torch Tensor) batch of input images\n",
    "        labels     (torch Tensor) ground truth labels\n",
    "        criterion   loss function\n",
    "        optimizer  used in backward pass\n",
    "\n",
    "    Returns:\n",
    "        running_loss    (float) loss from this batch of images\n",
    "        num_correct     (torch Tensor, size 1) number of inputs\n",
    "                        in this batch predicted correctly\n",
    "        total_images    (float or int) total number of images in this batch\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with torch.set_grad_enabled(True):\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "    \n",
    "    # back pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, index_out = outputs.max(1)\n",
    "    running_loss = loss.item()\n",
    "    num_correct = sum(index_out.squeeze() == labels.squeeze()).item()\n",
    "    total_images = inputs.size(0)\n",
    "\n",
    "    return running_loss, num_correct, total_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the params of our net\n",
    "# !pip install torchsummary\n",
    "# from torchsummary import summary\n",
    "# summary(net, input_size=(2,64, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(curr_batch, batch_size, curr_epoch, total_epochs, dataloader):\n",
    "    \"\"\"\n",
    "    Prints message logging progress through training.\n",
    "    \"\"\"\n",
    "    progress = float(curr_batch + 1)/(float(len(dataloader.dataset)) / batch_size)\n",
    "    log = \"EPOCH [{}/{}].Progress: {} % \".format(\n",
    "        curr_epoch + 1, total_epochs, round(progress * 100, 2))\n",
    "    sys.stdout.write(\"\\r\" + log)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # test dataloader\n",
    "# for batch_num, (test_inputs, test_labels) in enumerate(test_dataloader):\n",
    "#     print(\"batch_num = \" + str(batch_num))\n",
    "#     print(\"label = \" + str(test_labels))\n",
    "#     print(test_inputs)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "EPOCH [1/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [6.9245] Acc: 0.5379464285714286 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.6516] Acc: 0.59375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [2/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.2631] Acc: 0.5089285714285714 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.1148] Acc: 0.375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [3/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0311] Acc: 0.5513392857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0252] Acc: 0.5625 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [4/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0131] Acc: 0.5580357142857143 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0209] Acc: 0.53125 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [5/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0101] Acc: 0.6294642857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0182] Acc: 0.65625 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [6/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.01] Acc: 0.6138392857142857 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0171] Acc: 0.84375 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [7/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0094] Acc: 0.640625 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.017] Acc: 0.8125 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [8/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0091] Acc: 0.6808035714285714 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0144] Acc: 0.8125 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [9/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0086] Acc: 0.7142857142857143 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.0132] Acc: 1.0 on 32.0 images\n",
      "\n",
      "Training...\n",
      "EPOCH [10/10].Progress: 91.62 % \n",
      " Train Avg. Loss: [0.0087] Acc: 0.7299107142857143 on 448.0 images\n",
      "\n",
      "Validating...\n",
      "Val Avg. Loss: [0.011] Acc: 0.96875 on 32.0 images\n",
      "\n",
      "Time Elapsed: 453.851432 seconds\n"
     ]
    }
   ],
   "source": [
    "train_loss_history = np.zeros(epochs)\n",
    "train_acc_history  = np.zeros(epochs)\n",
    "val_loss_history   = np.zeros(epochs)\n",
    "val_acc_history    = np.zeros(epochs)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # TRAINING ###\n",
    "    print(\"Training...\")\n",
    "    net.train()\n",
    "    \n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "\n",
    "    for batch_num, (inputs, labels) in enumerate(train_dataloader):\n",
    "        #print(labels)\n",
    "        log_progress(batch_num, BATCH_SIZE, epoch, epochs, train_dataloader)\n",
    "\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        assert inputs.shape == (64,3,224,224), inputs.shape\n",
    "        curr_loss, curr_correct, curr_images = model_train(net, inputs, labels, criterion, optimizer)\n",
    "        running_loss += curr_loss\n",
    "        num_correct += curr_correct\n",
    "        total_images += curr_images\n",
    "\n",
    "    # Update statistics for epoch\n",
    "    train_loss_history[epoch] = running_loss / total_images\n",
    "    train_acc_history[epoch]  = float(num_correct)  / float(total_images)\n",
    "    print(\"\\n Train Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "          round(train_loss_history[epoch],4), train_acc_history[epoch], total_images) )\n",
    "    \n",
    "    # VALIDATION ###\n",
    "\n",
    "    print(\"Validating...\")\n",
    "    net.eval()\n",
    "    \n",
    "    running_loss   = 0.0\n",
    "    num_correct    = 0.0\n",
    "    total_images   = 0.0\n",
    "    \n",
    "    for batch_num, (test_inputs, test_labels) in enumerate(test_dataloader):\n",
    "        \n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            test_inputs = test_inputs.cuda()\n",
    "            test_labels = test_labels.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs  = net(test_inputs)\n",
    "                                                 \n",
    "        loss     = criterion(outputs, test_labels.squeeze())\n",
    "                                                 \n",
    "        # Prediction is index with highest class score\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # move data back for analysis\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "            test_labels = test_labels.cpu()\n",
    "            preds = preds.cpu()\n",
    "\n",
    "        running_loss  += loss.item()\n",
    "        num_correct   += torch.sum(preds == test_labels.data.reshape(-1))\n",
    "\n",
    "        total_images  += test_labels.data.numpy().size\n",
    "        \n",
    "    # Update stats for validation data\n",
    "    val_loss_history[epoch] = running_loss / total_images\n",
    "    val_acc_history[epoch]  = float(num_correct)  / float(total_images) \n",
    "    print(\"Val Avg. Loss: [{}] Acc: {} on {} images\\n\".format(\n",
    "        round(val_loss_history[epoch],4), val_acc_history[epoch], total_images))\n",
    "    \n",
    "print(\"Time Elapsed: {} seconds\".format(\n",
    "    (datetime.now() - start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cough_net = net\n",
    "cough_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_non_cough_idx = train_labels[train_labels.iloc[:,1] != 'Cough'].index\n",
    "train_cough_idx = train_labels[train_labels.iloc[:,1] == 'Cough'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = FreesoundDataset(csv_labels='train_binary_labels.csv',root_dir='.',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0])\n",
      "labels = tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "cough_net.eval()\n",
    "\n",
    "for batch_num, (test_inputs, test_labels) in enumerate(test_dataloader):\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "            test_inputs = test_inputs.cuda()\n",
    "            test_labels = test_labels.cuda()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs  = net(test_inputs)\n",
    "                                                 \n",
    "    loss     = criterion(outputs, test_labels.squeeze())\n",
    "                                                 \n",
    "    # Prediction is index with highest class score\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    print(\"predictions = \" + str(preds))\n",
    "    print(\"labels = \" + str(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "fs = 44100  # Sample rate\n",
    "seconds = 3  # Duration of recording\n",
    "\n",
    "myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=1)\n",
    "sd.wait()  # Wait until recording is finished\n",
    "write('output.wav', fs, myrecording)  # Save as WAV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        , ..., -0.00117799,\n",
       "       -0.00129872, -0.00133156], dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rate, sample = wavfile.read('output.wav')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "ename": "ParameterError",
     "evalue": "Audio buffer is not finite everywhere",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParameterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-02b83d679c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-98470a6afaeb>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      7\u001b[0m transform = transforms.Compose([\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     ])\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mmfcc\u001b[0;34m(y, sr, S, n_mfcc, dct_type, norm, lifter, **kwargs)\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mS\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpower_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfftpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdct_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_mfcc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   1815\u001b[0m     S, n_fft = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length, power=power,\n\u001b[1;32m   1816\u001b[0m                             \u001b[0mwin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                             pad_mode=pad_mode)\n\u001b[0m\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;31m# Build a Mel filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36m_spectrogram\u001b[0;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[1;32m   2526\u001b[0m         S = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length,\n\u001b[1;32m   2527\u001b[0m                         \u001b[0mwin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2528\u001b[0;31m                         window=window, pad_mode=pad_mode))**power\n\u001b[0m\u001b[1;32m   2529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m# Check audio is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Pad the time series so that frames are centered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/util/utils.py\u001b[0m in \u001b[0;36mvalid_audio\u001b[0;34m(y, mono)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mParameterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Audio buffer is not finite everywhere'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"F_CONTIGUOUS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParameterError\u001b[0m: Audio buffer is not finite everywhere"
     ]
    }
   ],
   "source": [
    "sample_rate, sample = wavfile.read('output.wav')\n",
    "sample = sample + 1e-012\n",
    "# start_time = datetime.now()\n",
    "# clamp all samples to 500,000 frames\n",
    "if len(sample) > 500000:\n",
    "    sample = sample[:500000]\n",
    "else:\n",
    "    l = len(sample)\n",
    "    temp = np.zeros(500000,dtype='int16')\n",
    "    temp[0:l] = sample\n",
    "    sample = temp\n",
    "    \n",
    "print(np.isnan(sample).any())\n",
    "sample = transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "ename": "ParameterError",
     "evalue": "Audio buffer is not finite everywhere",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParameterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-dcb378486042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlive_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-6fa054351db5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# reshape to (3,224,224) for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m977\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-98470a6afaeb>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      7\u001b[0m transform = transforms.Compose([\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_mfcc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     ])\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mmfcc\u001b[0;34m(y, sr, S, n_mfcc, dct_type, norm, lifter, **kwargs)\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mS\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpower_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfftpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdct_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_mfcc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/feature/spectral.py\u001b[0m in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   1815\u001b[0m     S, n_fft = _spectrogram(y=y, S=S, n_fft=n_fft, hop_length=hop_length, power=power,\n\u001b[1;32m   1816\u001b[0m                             \u001b[0mwin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                             pad_mode=pad_mode)\n\u001b[0m\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[0;31m# Build a Mel filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36m_spectrogram\u001b[0;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[1;32m   2526\u001b[0m         S = np.abs(stft(y, n_fft=n_fft, hop_length=hop_length,\n\u001b[1;32m   2527\u001b[0m                         \u001b[0mwin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2528\u001b[0;31m                         window=window, pad_mode=pad_mode))**power\n\u001b[0m\u001b[1;32m   2529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2530\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/core/spectrum.py\u001b[0m in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;31m# Check audio is valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;31m# Pad the time series so that frames are centered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/librosa/util/utils.py\u001b[0m in \u001b[0;36mvalid_audio\u001b[0;34m(y, mono)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mParameterError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Audio buffer is not finite everywhere'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"F_CONTIGUOUS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParameterError\u001b[0m: Audio buffer is not finite everywhere"
     ]
    }
   ],
   "source": [
    "sample_rate, sample = wavfile.read('output.wav')\n",
    "sample = sample + 1e-012\n",
    "# start_time = datetime.now()\n",
    "# clamp all samples to 500,000 frames\n",
    "if len(sample) > 500000:\n",
    "    sample = sample[:500000]\n",
    "else:\n",
    "    l = len(sample)\n",
    "    temp = np.zeros(500000,dtype='int16')\n",
    "    temp[0:l] = sample\n",
    "    sample = temp\n",
    "    \n",
    "sample = sample + 1e-012\n",
    "\n",
    "sample = transform(sample)\n",
    "# reshape to (3,224,224) for\n",
    "temp = torch.tensor(sample.numpy().reshape(1,1,128,977))\n",
    "downsample = torch.nn.functional.interpolate(temp, (224, 224), mode='bilinear')\n",
    "temp = np.repeat(downsample[...],3, axis=1)\n",
    "sample = temp.squeeze()\n",
    "normalize = transforms.Compose([transforms.Normalize(mean,std)])\n",
    "sample = normalize(sample)\n",
    "#print(\"Time Elapsed Processing WAV: {} seconds\".format(\n",
    "#(datetime.now() - start_time).total_seconds()))\n",
    "sample.shape\n",
    "'''Not sure if anything from here on works'''\n",
    "live_data = FreesoundDataset(csv_labels = 'live_binary_labels.csv', root_dir='.',transform=transform, test=True, evaluate=True)\n",
    "live_dataloader = DataLoader(live_data, batch_size=1, shuffle=True, drop_last=True)\n",
    "preds = []\n",
    "labels = []\n",
    "for batch_num, (input, label) in enumerate(live_dataloader):\n",
    "    output = net(input.float())\n",
    "    _, pred = torch.max(output, 1)\n",
    "    preds.append(pred)\n",
    "    labels.append(label)\n",
    "    break\n",
    "print(\"Cough was predicted as...\")\n",
    "\n",
    "if pred.numpy()[0]:\n",
    "    print(\"Cough\")\n",
    "else:\n",
    "    print(\"Not a Cough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
